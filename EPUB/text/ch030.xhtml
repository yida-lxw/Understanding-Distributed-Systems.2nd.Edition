<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-GB">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <title>ch030.xhtml</title>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css" />
</head>
<body epub:type="bodymatter">
<section id="filestorage" class="level1" data-number="17">
<h1 data-number="17"><span class="header-section-number">17</span> File storage</h1>
<p>Using a CDN has significantly reduced the number of requests hitting <em>Cruder</em>’s application server. But there are only so many images, videos, etc., the server can store on its local disk(s) before running out of space. To work around this limit, we can use a managed file store, like AWS S3<a href="#fn1" class="footnote-ref" id="fnref1" epub:type="noteref">1</a> or Azure Blob Storage<a href="#fn2" class="footnote-ref" id="fnref2" epub:type="noteref">2</a>, to store large static files. Managed file stores are scalable, highly available, and offer strong durability guarantees. A file uploaded to a managed store can be configured to allow access to anyone who knows its URL, which means we can point the CDN straight at it. This allows us to completely offload the storage and serving of static resources to managed services.</p>
<section id="blob-storage-architecture" class="level2" data-number="17.1">
<h2 data-number="17.1"><span class="header-section-number">17.1</span> Blob storage architecture</h2>
<p>Because distributed file stores are such a crucial component of modern applications, it’s useful to have an idea of how they work underneath. In this chapter, we will dive into the architecture of Azure Storage<a href="#fn3" class="footnote-ref" id="fnref3" epub:type="noteref">3</a> (AS), a scalable cloud storage system that provides strong consistency. AS supports file, queue, and table abstractions, but for simplicity, our discussion will focus exclusively on the file abstraction, also referred to as the blob store.</p>
<p>AS is composed of storage clusters distributed across multiple regions worldwide. A <em>storage cluster</em> is composed of multiple racks of nodes, where each rack is built out as a separate unit with redundant networking and power.</p>
<p>At a high level, AS exposes a global namespace based on domain names that are composed of two parts: an account name and a file name. The two names together form a unique URL that points to a specific file, e.g., <em><a href="https://ACCOUNT_NAME.blob.core.windows.net/FILE_NAME" class="uri">https://ACCOUNT_NAME.blob.core.windows.net/FILE_NAME</a></em>. The customer configures the account name, and the AS DNS server uses it to identify the storage cluster where the data is stored. The cluster uses the file name to locate the node responsible for the data.</p>
<p>A central <em>location service</em> acts as the global <em>control plane</em> in charge of creating new accounts and allocating them to clusters, and also moving them from one cluster to another for better load distribution. For example, when a customer wants to create a new account in a specific region, the location service:</p>
<ul>
<li>chooses a suitable cluster to which to allocate the account based on load information;</li>
<li>updates the configuration of the cluster to start accepting requests for the new account;</li>
<li>and creates a new DNS record that maps the account name to the cluster’s public IP address.</li>
</ul>
<p>From an architectural point of view, a storage cluster is composed of three layers: a stream layer, a partition layer, and a front-end layer (see Figure <a href="#fig:aslayers">17.1</a>).</p>
<div class="figure" style="text-align: center">
<img alt="A high-level view of Azure Storage&#39;s architecture" width="80%" src="../media/file44.png" />
<p class="caption">
Figure 17.1: A high-level view of Azure Storage’s architecture
</p>
</div>
<p>The <em>stream layer</em> implements a distributed append-only file system in which the data is stored in so-called streams. Internally, a <em>stream</em> is represented as a sequence of <em>extents</em>, where the extent is the unit of replication. Writes to extents are replicated synchronously using chain replication<a href="#fn4" class="footnote-ref" id="fnref4" epub:type="noteref">4</a>.</p>
<p>The <em>stream manager</em> is the control plane responsible for assigning an extent to a chain of storage servers in the cluster. When the manager is asked to allocate a new extent, it replies with the list of storage servers that hold a copy of the newly created extent (see Figure <a href="#fig:astream">17.2</a>). The client caches this information and uses it to send future writes to the primary server. The stream manager is also responsible for handling unavailable or faulty extent replicas by creating new ones and reconfiguring the replication chains they are part of.</p>
<div class="figure" style="text-align: center">
<img alt="The stream layer uses chain replication to replicate extents across storage servers." width="100%" src="../media/file45.png" />
<p class="caption">
Figure 17.2: The stream layer uses chain replication to replicate extents across storage servers.
</p>
</div>
<p>The <em>partition layer</em> is where high-level file operations are translated to low-level stream operations. Within this layer, the <em>partition manager</em> (yet another control plane) manages a large index of all files stored in the cluster. Each entry in the index contains metadata such as account and file name and a pointer to the actual data in the stream service (list of extent plus offset and length). The partition manager range-partitions the index and maps each partition to a partition server. The partition manager is also responsible for load-balancing partitions across servers, splitting partitions when they become too hot, and merging cold ones (see Figure <a href="#fig:aspartition">17.3</a>).</p>
<p>The partition layer also asynchronously replicates accounts across clusters in the background. This functionality is used to migrate accounts from one cluster to another for load-balancing purposes and disaster recovery.</p>
<div class="figure" style="text-align: center">
<img alt="The partition manager range-partitions files across partition servers and rebalances the partitions when necessary." width="100%" src="../media/file46.png" />
<p class="caption">
Figure 17.3: The partition manager range-partitions files across partition servers and rebalances the partitions when necessary.
</p>
</div>
<p>Finally, the <em>front-end service</em> (a reverse proxy) is a stateless service that authenticates requests and routes them to the appropriate partition server using the mapping managed by the partition manager.</p>
<p>Although we have only coarsely described the architecture of AS, it’s a great showcase of the scalability patterns applied to a concrete system. As an interesting historical note, AS was built from the ground up to be strongly consistent, while AWS S3 started offering the same guarantee in 2021<a href="#fn5" class="footnote-ref" id="fnref5" epub:type="noteref">5</a>.</p>
</section>
</section>
<section class="footnotes" epub:type="footnotes">
<hr />
<ol>
<li id="fn1" epub:type="footnote"><p>“Amazon Simple Storage Service,” <a href="https://aws.amazon.com/s3/" class="uri">https://aws.amazon.com/s3/</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" epub:type="footnote"><p>“Azure Blob Storage,” <a href="https://azure.microsoft.com/en-us/services/storage/blobs/#overview" class="uri">https://azure.microsoft.com/en-us/services/storage/blobs/#overview</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" epub:type="footnote"><p>“Windows Azure Storage: A Highly Available Cloud Storage Service with Strong Consistency,” <a href="https://sigops.org/s/conferences/sosp/2011/current/2011-Cascais/printable/11-calder.pdf" class="uri">https://sigops.org/s/conferences/sosp/2011/current/2011-Cascais/printable/11-calder.pdf</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" epub:type="footnote"><p>we discussed chain replication in section <a href="#chain-replication">10.4</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" epub:type="footnote"><p>“Diving Deep on S3 Consistency,” <a href="https://www.allthingsdistributed.com/2021/04/s3-strong-consistency.html" class="uri">https://www.allthingsdistributed.com/2021/04/s3-strong-consistency.html</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
