<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-GB">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <title>ch031.xhtml</title>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css" />
</head>
<body epub:type="bodymatter">
<section id="load-balancing" class="level1" data-number="18">
<h1 data-number="18"><span class="header-section-number">18</span> Network load balancing</h1>
<p>By offloading requests to the file store and the CDN, <em>Cruder</em> is able to serve significantly more requests than before. But the free lunch is only going to last so long. Because there is a single application server, it will inevitably fall over if the number of requests directed at it keeps increasing. To avoid that, we can create multiple application servers, each running on a different machine, and have a <em>load balancer</em> distribute requests to them. The thinking is that if one server has a certain capacity, then, in theory, two servers should have twice that capacity. This is an example of the more general scalability pattern we referred to as scaling out or scaling horizontally.</p>
<p>The reason we can scale <em>Cruder</em> horizontally is that we have pushed the state to dedicated services (the database and the managed file store). Scaling out a stateless application doesn’t require much effort, <em>assuming</em> its dependencies can scale accordingly as well. As we will discuss in the next chapter, scaling out a stateful service, like a data store, is a lot more challenging since it needs to replicate state and thus requires some form of coordination, which adds complexity and can also become a bottleneck. As a general rule of thumb, we should try to keep our applications stateless by pushing state to third-party services designed by teams with years of experience building such services.</p>
<p>Distributing requests across a pool of servers has many benefits. Because clients are decoupled from servers and don’t need to know their individual addresses, the number of servers behind the load balancer can increase or decrease transparently. And since multiple redundant servers can interchangeably be used to handle requests, a load balancer can detect faulty ones and take them out of the pool, increasing the availability of the overall application.</p>
<p>As you might recall from chapter <a href="#introduction">1</a>, the availability of a system is the percentage of time it’s capable of servicing requests and doing useful work. Another way of thinking about it is that it’s the probability that a request will succeed.</p>
<p>The reason why a load balancer increases the theoretical availability is that in order for the application to be considered unavailable, all the servers need to be down. With N servers, the probability that they are all unavailable is the product of the servers’ failure rates<a href="#fn1" class="footnote-ref" id="fnref1" epub:type="noteref">1</a>. By subtracting this product from 1, we can determine the theoretical availability of the application.</p>
<p>For example, if we have two servers behind a load balancer and each has an availability of 99%, then the application has a theoretical availability of 99.99%:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.01</mn><mo>⋅</mo><mn>0.01</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0.9999</mn></mrow><annotation encoding="application/x-tex">
1 - (0.01 \cdot 0.01) = 0.9999
</annotation></semantics></math></p>
<p>Intuitively, the nines of independent servers sum up.<a href="#fn2" class="footnote-ref" id="fnref2" epub:type="noteref">2</a> Thus, in the previous example, we have two independent servers with two nines each, for a total of four nines of availability. Of course, this number is only theoretical because, in practice, the load balancer doesn’t remove faulty servers from the pool immediately. The formula also naively assumes that the failure rates are independent, which might not be the case. Case in point: when a faulty server is removed from the load balancer’s pool, the remaining ones might not be able to sustain the increase in load and degrade.</p>
<p>In the following sections, we will take a closer look at some of the core features offered by a load balancer.</p>
<p><strong>Load balancing</strong></p>
<p>The algorithms used for routing requests can vary from round-robin to consistent hashing to ones that take into account the servers’ load.</p>
<p>As a fascinating side note, balancing by load is a lot more challenging than it seems in a distributed context. For example, the load balancer could periodically sample a dedicated <em>load endpoint</em> exposed by each server that returns a measure of how busy the server is (e.g., CPU usage). And since constantly querying servers can be costly, the load balancer can cache the responses for some time.</p>
<p>Using cached or otherwise delayed metrics to distribute requests to servers can result in surprising behaviors. For example, if a server that just joined the pool reports a load of 0, the load balancer will hammer it until the next time its load is sampled. When that happens, the server will report that it’s overloaded, and the load balancer will stop sending more requests to it. This causes the server to alternate between being very busy and not being busy at all.</p>
<p>As it turns out, randomly distributing requests to servers without accounting for their load achieves a better load distribution. Does that mean that load balancing using delayed load metrics is not possible? There is a way, but it requires combining load metrics with the power of randomness. The idea is to randomly pick two servers from the pool and route the request to the least-loaded one of the two. This approach works remarkably well in practice<a href="#fn3" class="footnote-ref" id="fnref3" epub:type="noteref">3</a>.</p>
<p><strong>Service discovery</strong></p>
<p>Service discovery is the mechanism the load balancer uses to discover the pool of servers it can route requests to. A naive way to implement it is to use a static configuration file that lists the IP addresses of all the servers, which is painful to manage and keep up to date.</p>
<p>A more flexible solution is to have a fault-tolerant coordination service, like, e.g., etcd or Zookeeper, manage the list of servers. When a new server comes online, it registers itself to the coordination service with a TTL. When the server unregisters itself, or the TTL expires because it hasn’t renewed its registration, the server is removed from the pool.</p>
<p>Adding and removing servers dynamically from the load balancer’s pool is a key functionality cloud providers use to implement autoscaling<a href="#fn4" class="footnote-ref" id="fnref4" epub:type="noteref">4</a>, i.e., the ability to spin up and tear down servers based on load.</p>
<p><strong>Health checks</strong></p>
<p>A load balancer uses health checks to detect when a server can no longer serve requests and needs to be temporarily removed from the pool. There are fundamentally two categories of health checks: passive and active.</p>
<p>A <em>passive health check</em> is performed by the load balancer as it routes incoming requests to the servers downstream. If a server isn’t reachable, the request times out, or the server returns a non-retriable status code (e.g., <code>503</code>), the load balancer can decide to take that server out of the pool.</p>
<p>Conversely, an <em>active health check</em> requires support from the downstream servers, which need to expose a dedicated <em>health endpoint</em> that the load balancer can query periodically to infer the server’s health. The endpoint returns <em>200 (OK)</em> if the server can serve requests or a 5xx status code if it’s overloaded and doesn’t have more capacity to serve requests. If a request to the endpoint times out, it also counts as an error.</p>
<p>The endpoint’s handler could be as simple as always returning <em>200 OK</em>, since most requests will time out when the server is degraded. Alternatively, the handler can try to infer whether the server is degraded by comparing local metrics, like CPU usage, available memory, or the number of concurrent requests being served, with configurable thresholds.</p>
<p>But here be dragons<a href="#fn5" class="footnote-ref" id="fnref5" epub:type="noteref">5</a>: if a threshold is misconfigured or the health check has a bug, all the servers behind the load balancer may fail the health check. In that case, the load balancer could naively empty the pool, taking the application down. However, in practice, if the load balancer is “smart enough,” it should detect that a large fraction of the servers are unhealthy and consider the health checks to be unreliable. So rather than removing servers from the pool, it should ignore the health checks altogether so that new requests can be sent to any server.</p>
<p>Thanks to health checks, the application behind the load balancer can be updated to a new version without any downtime. During the update, a rolling number of servers report themselves as unavailable so that the load balancer stops sending requests to them. This allows in-flight requests to complete (drain) before the servers are restarted with the new version. More generally, we can use this mechanism to restart a server without causing harm.</p>
<p>For example, suppose a stateless application has a rare memory leak that causes a server’s available memory to decrease slowly over time. When the server has very little physical memory available, it will swap memory pages to disk aggressively. This constant swapping is expensive and degrades the performance of the server dramatically. Eventually, the leak will affect the majority of servers and cause the application to degrade.</p>
<p>In this case, we could force a severely degraded server to restart. That way, we don’t have to develop complex recovery logic when a server gets into a rare and unexpected degraded mode. Moreover, restarting the server allows the system to self-heal, giving its operators time to identify the root cause.</p>
<p>To implement this behavior, a server could have a separate background thread — a <em>watchdog</em> — that wakes up periodically and monitors the server’s health. For example, the watchdog could monitor the available physical memory left. When a monitored metric breaches a specific threshold for some time, the watchdog considers the server degraded and deliberately crashes or restarts it.</p>
<p>Of course, the watchdog’s implementation needs to be well-tested and monitored since a bug could cause servers to restart continuously. That said, I find it uncanny how this simple pattern can make an application a lot more robust to gray failures.</p>
<section id="dns-load-balancing" class="level2" data-number="18.1">
<h2 data-number="18.1"><span class="header-section-number">18.1</span> DNS load balancing</h2>
<p>Now that we are familiar with the job description of a load balancer, let’s take a closer look at how it can be implemented. While you won’t have to build your own load balancer given the abundance of off-the-shelf solutions available, it’s important to have a basic knowledge of how a load balancer works. Because every request needs to go through it, it contributes to your applications’ performance and availability.</p>
<p>A simple way to implement a load balancer is with DNS. For example, suppose we have a couple of servers that we would like to load-balance requests over. If these servers have public IP addresses, we can add those to the application’s DNS record and have the clients pick one<a href="#fn6" class="footnote-ref" id="fnref6" epub:type="noteref">6</a> when resolving the DNS address, as shown in Figure <a href="#fig:dnsbalancing">18.1</a>.</p>
<div class="figure" style="text-align: center">
<img alt="DNS load balancing" width="60%" src="../media/file47.png" />
<p class="caption">
Figure 18.1: DNS load balancing
</p>
</div>
<p>Although this approach works, it’s not resilient to failures. If one of the two servers goes down, the DNS server will happily continue to serve its IP address, unaware that it’s no longer available. Even if we were to automatically reconfigure the DNS record when a failure happens and take out the problematic IP, the change needs time to propagate to the clients, since DNS entries are cached, as discussed in chapter <a href="#dns">4</a>.</p>
<p>The one use case where DNS is used in practice to load-balance is for distributing traffic to different data centers located in different regions (<em>global DNS load balancing</em>). We have already encountered a use for this when discussing CDNs.</p>
</section>
<section id="transport-layer-load-balancing" class="level2" data-number="18.2">
<h2 data-number="18.2"><span class="header-section-number">18.2</span> Transport layer load balancing</h2>
<p>A more flexible load-balancing solution can be implemented with a load balancer that operates at the TCP level of the network stack (aka L4 load balancer<a href="#fn7" class="footnote-ref" id="fnref7" epub:type="noteref">7</a>) through which all the traffic between clients and servers flows.</p>
<p>A network load balancer has one or more physical <em>network interface cards</em> mapped to one or more <em>virtual IP</em> (VIP) addresses. A VIP, in turn, is associated with a pool of servers. The load balancer acts as an intermediary between clients and servers — clients only see the VIP exposed by the load balancer and have no visibility of the individual servers associated with it.</p>
<p>When a client creates a new TCP connection with a load balancer’s VIP, the load balancer picks a server from the pool and henceforth shuffles the packets back and forth for that connection between the client and the server. And because all the traffic goes through the load balancer, it can detect servers that are unavailable (e.g., with a passive health check) and automatically take them out of the pool, improving the system’s reliability.</p>
<p>A connection is identified by a tuple (source IP/port, destination IP/port). Typically, some form of hashing is used to assign a connection tuple to a server that minimizes the disruption caused by a server being added or removed from the pool, like consistent hashing<a href="#fn8" class="footnote-ref" id="fnref8" epub:type="noteref">8</a>.</p>
<p>To forward packets downstream, the load balancer translates<a href="#fn9" class="footnote-ref" id="fnref9" epub:type="noteref">9</a> each packet’s source address to the load balancer’s address and its destination address to the server’s address. Similarly, when the load balancer receives a packet from the server, it translates its source address to the load balancer’s address and its destination address to the client’s address (see Figure <a href="#fig:l4balancing">18.2</a>).</p>
<div class="figure" style="text-align: center">
<img alt="Transport layer load balancing" width="60%" src="../media/file48.png" />
<p class="caption">
Figure 18.2: Transport layer load balancing
</p>
</div>
<p>As the data going out of the servers usually has a greater volume than the data coming in, there is a way for servers to bypass the load balancer and respond directly to the clients using a mechanism called direct server return<a href="#fn10" class="footnote-ref" id="fnref10" epub:type="noteref">10</a>, which can significantly reduce the load on the load balancer.</p>
<p>A network load balancer can be built using commodity machines and scaled out using a combination of <em>Anycast</em><a href="#fn11" class="footnote-ref" id="fnref11" epub:type="noteref">11</a> and <em>ECMP</em><a href="#fn12" class="footnote-ref" id="fnref12" epub:type="noteref">12</a>. Load balancer instances announce themselves to the data center’s edge routers with the same Anycast VIP and identical BGP weight. Using an Anycast IP is a neat trick that allows multiple machines to share the same IP address and have routers send traffic to the one with the lowest BGP weight. If all the instances have the same identical BGP weight, routers use equal-cost multi-path routing (consistent hashing) to ensure that the packets of a specific connection are generally routed to the same load balancer instance.</p>
<p>Since <em>Cruder</em> is hosted in the cloud, we can leverage one of the many managed solutions for network load balancing, such as AWS Network Load Balancer<a href="#fn13" class="footnote-ref" id="fnref13" epub:type="noteref">13</a> or Azure Load Balancer<a href="#fn14" class="footnote-ref" id="fnref14" epub:type="noteref">14</a>.</p>
<p>Although load balancing connections at the TCP level is very fast, the drawback is that the load balancer is just shuffling bytes around without knowing what they actually mean. Therefore, L4 load balancers generally don’t support features that require higher-level network protocols, like terminating TLS connections. A load balancer that operates at a higher level of the network stack is required to support these advanced use cases.</p>
</section>
<section id="l7lb" class="level2" data-number="18.3">
<h2 data-number="18.3"><span class="header-section-number">18.3</span> Application layer load balancing</h2>
<p>An application layer load balancer (aka L7 load balancer<a href="#fn15" class="footnote-ref" id="fnref15" epub:type="noteref">15</a>) is an HTTP reverse proxy that distributes requests over a pool of servers. The load balancer receives an HTTP request from a client, inspects it, and sends it to a backend server.</p>
<p>There are two different TCP connections at play here, one between the client and the L7 load balancer and another between the L7 load balancer and the server. Because a L7 load balancer operates at the HTTP level, it can de-multiplex individual HTTP requests sharing the same TCP connection. This is even more important with HTTP 2, where multiple concurrent streams are multiplexed on the same TCP connection, and some connections can be a lot more expensive to handle than others.</p>
<p>The load balancer can do smart things with application traffic, like rate-limit requests based on HTTP headers, terminate TLS connections, or force HTTP requests belonging to the same <em>logical session</em> to be routed to the same backend server. For example, the load balancer could use a cookie to identify which logical session a request belongs to and map it to a server using consistent hashing. That allows servers to cache session data in memory and avoid fetching it from the data store for each request. The caveat is that sticky sessions can create hotspots, since some sessions can be much more expensive to handle than others.</p>
<p>A L7 load balancer can be used as the backend of a L4 load balancer that load-balances requests received from the internet. Although L7 load balancers have more capabilities than L4 load balancers, they also have lower throughput, making L4 load balancers better suited to protect against certain DDoS attacks, like SYN floods<a href="#fn16" class="footnote-ref" id="fnref16" epub:type="noteref">16</a>.</p>
<p>A drawback of using a dedicated load balancer is that all the traffic directed to an application needs to go through it. So if the load balancer goes down, the application behind it does too. However, if the clients are internal to the organization, load balancing can be delegated to them using the <em>sidecar pattern</em>. The idea is to proxy all a client’s network traffic through a process co-located on the same machine (the sidecar proxy). The sidecar process acts as a L7 load balancer, load-balancing requests to the right servers. And, since it’s a reverse proxy, it can also implement various other functions, such as rate-limiting, authentication, and monitoring.</p>
<p>This approach<a href="#fn17" class="footnote-ref" id="fnref17" epub:type="noteref">17</a> (aka “service mesh”) has been gaining popularity with the rise of microservices in organizations with hundreds of services communicating with each other. As of this writing, popular sidecar proxy load balancers are NGINX, HAProxy, and Envoy. The main advantage of this approach is that it delegates load-balancing to the clients, removing the need for a dedicated load balancer that needs to be scaled out and maintained. The drawback is that it makes the system a lot more complex since now we need a control plane to manage all the sidecars<a href="#fn18" class="footnote-ref" id="fnref18" epub:type="noteref">18</a>.</p>
</section>
</section>
<section class="footnotes" epub:type="footnotes">
<hr />
<ol>
<li id="fn1" epub:type="footnote"><p>“AWS Well-Architected Framework, Availability,” <a href="https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/availability.html" class="uri">https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/availability.html</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" epub:type="footnote"><p>Another way to think about it is that by increasing the number of servers linearly, we increase the availability exponentially.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" epub:type="footnote"><p>“The power of two random choices,” <a href="https://brooker.co.za/blog/2012/01/17/two-random.html" class="uri">https://brooker.co.za/blog/2012/01/17/two-random.html</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" epub:type="footnote"><p>“Autoscaling,” <a href="https://docs.microsoft.com/en-us/azure/architecture/best-practices/auto-scaling" class="uri">https://docs.microsoft.com/en-us/azure/architecture/best-practices/auto-scaling</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" epub:type="footnote"><p>“Implementing health checks,” <a href="https://aws.amazon.com/builders-library/implementing-health-checks/" class="uri">https://aws.amazon.com/builders-library/implementing-health-checks/</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" epub:type="footnote"><p>“Round-robin DNS,” <a href="https://en.wikipedia.org/wiki/Round-robin_DNS" class="uri">https://en.wikipedia.org/wiki/Round-robin_DNS</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" epub:type="footnote"><p>layer 4 is the transport layer in the OSI model<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" epub:type="footnote"><p>“SREcon19 Americas - Keeping the Balance: Internet-Scale Loadbalancing Demystified,” <a href="https://www.youtube.com/watch?v=woaGu3kJ-xk" class="uri">https://www.youtube.com/watch?v=woaGu3kJ-xk</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" epub:type="footnote"><p>“Network address translation,” <a href="https://en.wikipedia.org/wiki/Network_address_translation" class="uri">https://en.wikipedia.org/wiki/Network_address_translation</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" epub:type="footnote"><p>“Introduction to modern network load balancing and proxying,” <a href="https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236" class="uri">https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236</a><a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" epub:type="footnote"><p>“Anycast,” <a href="https://en.wikipedia.org/wiki/Anycast" class="uri">https://en.wikipedia.org/wiki/Anycast</a><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12" epub:type="footnote"><p>“Equal-cost multi-path routing,” <a href="https://en.wikipedia.org/wiki/Equal-cost_multi-path_routing" class="uri">https://en.wikipedia.org/wiki/Equal-cost_multi-path_routing</a><a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13" epub:type="footnote"><p>“Network Load Balancer,” <a href="https://aws.amazon.com/elasticloadbalancing/network-load-balancer/" class="uri">https://aws.amazon.com/elasticloadbalancing/network-load-balancer/</a><a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14" epub:type="footnote"><p>“Azure Load Balancer,” <a href="https://azure.microsoft.com/en-us/services/load-balancer/" class="uri">https://azure.microsoft.com/en-us/services/load-balancer/</a><a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15" epub:type="footnote"><p>layer 7 is the application layer in the OSI model<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16" epub:type="footnote"><p>A SYN flood is a form of denial-of-service attack in which an attacker rapidly initiates a TCP connection to a server without finalizing the connection.<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17" epub:type="footnote"><p>“Service mesh data plane vs. control plane,” <a href="https://blog.envoyproxy.io/service-mesh-data-plane-vs-control-plane-2774e720f7fc" class="uri">https://blog.envoyproxy.io/service-mesh-data-plane-vs-control-plane-2774e720f7fc</a><a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18" epub:type="footnote"><p>“Service Mesh Wars, Goodbye Istio,” <a href="https://blog.polymatic.systems/service-mesh-wars-goodbye-istio-b047d9e533c7" class="uri">https://blog.polymatic.systems/service-mesh-wars-goodbye-istio-b047d9e533c7</a><a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
